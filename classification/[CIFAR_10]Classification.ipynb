{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPjh-eSU-aYY"
      },
      "source": [
        "In this notebook, we explore several feature reduction techniques as part of our image classification task. These techniques are used to reduce the dimensionality of the data and improve the efficiency and effectiveness of our models.\n",
        "\n",
        "We have included the following feature reduction techniques:\n",
        "\n",
        "1. **Simple Standardization**:\n",
        "   - A preprocessing step using StandardScaler to standardize the data.\n",
        "\n",
        "2. **Principal Component Analysis (PCA)**:\n",
        "   - Standardization followed by PCA, reducing the dimensionality to 2 components.\n",
        "   - `PCA` is a linear technique that can effectively capture variance in the data.\n",
        "\n",
        "3. **Linear Discriminant Analysis (LDA)**:\n",
        "   - Standardization followed by LDA, reducing the dimensionality to 2 components.\n",
        "   - `LDA` is a supervised technique that aims to maximize class separability.\n",
        "\n",
        "4. **Neighborhood Components Analysis (NCA)**:\n",
        "   - Standardization followed by NCA, reducing the dimensionality to 2 components.\n",
        "   - `NCA` is another supervised technique that focuses on preserving the relative neighborhood relationships.\n",
        "\n",
        "In addition to these feature reduction techniques, we have employed the following model and optimization techniques:\n",
        "\n",
        "- **Dataset Used**:\n",
        "   - The dataset used for this exploration is the CIFAR-10 dataset. It is a well-known dataset for image classification and contains a wide range of images across multiple classes.\n",
        "\n",
        "- **Classification Model**:\n",
        "   - We utilized the KNeighborsClassifier as our classification model. This model is suitable for classification tasks and is well-suited for use with reduced-dimensional feature representations.\n",
        "\n",
        "- **Hyperparameter Optimization**:\n",
        "   - GridSearchCV was employed to find the optimal hyperparameters for our KNeighborsClassifier. This systematic search allows us to fine-tune the model's settings for improved performance.\n",
        "\n",
        "By including these techniques in our notebook, we aim to showcase the versatility of feature reduction methods and demonstrate the impact of different models and hyperparameter tuning on image classification tasks using the CIFAR-10 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1voUSGY-cin",
        "outputId": "89948c74-0b5d-4023-a502-7087dd8bbd37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3)\n",
            "y_test shape: (10000, 1)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# x_train and x_test contain the images, y_train and y_test contain the labels\n",
        "\n",
        "# Look at the shape\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "SfYbtMFOIFH4",
        "outputId": "3d0bca7c-3865-4988-881d-f674a16409d2"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and holdout sets\n",
        "X_train_flattened, X_holdout_flattened, Y_train_flattened, Y_holdout_flattened = train_test_split(\n",
        "    [image.flatten() for image in x_train], y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the shapes of the resulting arrays\n",
        "print(\"X_train_flattened shape:\", np.array(X_train_flattened).shape)\n",
        "print(\"Y_train_flattened shape:\", Y_train_flattened.shape)\n",
        "print(\"X_holdout_flattened shape:\", np.array(X_holdout_flattened).shape)\n",
        "print(\"Y_holdout_flattened shape:\", Y_holdout_flattened.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cw8IOUSUFyf0"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import NeighborhoodComponentsAnalysis\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# simple standardisation\n",
        "scaler = make_pipeline(StandardScaler())\n",
        "\n",
        "# Reduce dimension with PCA\n",
        "pca = make_pipeline(StandardScaler(), PCA(n_components=2, random_state=43))\n",
        "\n",
        "# Reduce dimension with LinearDiscriminantAnalysis\n",
        "lda = make_pipeline(StandardScaler(), LinearDiscriminantAnalysis(n_components=2))\n",
        "\n",
        "# Reduce dimension with NeighborhoodComponentsAnalysis\n",
        "nca = make_pipeline(StandardScaler(), NeighborhoodComponentsAnalysis(n_components=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6-7GH7lq7_Q"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "steps = [\n",
        "    ('preprocessing', scaler),\n",
        "    ('classifier', knn),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Pkqf6-ipDKv",
        "outputId": "6ed8a964-be87-4f9c-a0b6-d308dab25284"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        }
      ],
      "source": [
        "# Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "grid_parameters = {\n",
        "    'classifier__n_neighbors': [5],\n",
        "    'classifier__weights': ['uniform'],\n",
        "    'classifier__leaf_size': [20],\n",
        "    'classifier__p': [2]\n",
        "}\n",
        "\n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "model = GridSearchCV(\n",
        "    estimator=pipeline,\n",
        "    param_grid=grid_parameters,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    verbose=2,\n",
        ")\n",
        "\n",
        "model.fit(X_train_flattened, Y_train_flattened)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmeie2KXrEPk"
      },
      "outputs": [],
      "source": [
        "# Scoring\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "scoring = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'precision': make_scorer(precision_score, average='weighted'),\n",
        "    'recall': make_scorer(recall_score, average='weighted'),\n",
        "    'f1': make_scorer(f1_score, average='weighted')\n",
        "}\n",
        "results = cross_validate(pipeline, X_train, Y_train, cv=5, scoring=scoring, return_train_score=True)\n",
        "\n",
        "accuracy_scores = results['test_accuracy']\n",
        "precision_scores = results['test_precision']\n",
        "recall_scores = results['test_recall']\n",
        "f1_scores = results['test_f1']\n",
        "\n",
        "\n",
        "print(f'Accuracy scores: {accuracy_scores}')\n",
        "print(f'Precision scores: {precision_scores}')\n",
        "print(f'Recall scores: {recall_scores}')\n",
        "print(f'F1 scores: {f1_scores}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
